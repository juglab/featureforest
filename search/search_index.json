{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Feature Forest Documentation","text":"<p>Feature Forest (FF) is a napari plugin for creating image annotations with less manual work, utilizing image embeddings (features) of vision transformer models like SAM2, and training a Random Forest model using a little scribble labels provided by the user.</p>"},{"location":"#overview","title":"Overview","text":"<p>FF plugin includes two widgets: Feature Extractor and Segmentation widgets.</p> <p></p> <p>Feature Extractor Widget</p> <p> </p> <p>Segmentation Widget</p>"},{"location":"feature_extractor/","title":"Feature Extractor Widget","text":"<p>After selecting your image stack, you need to extract the features. Later, these image features will be used as inputs for training a Random Forest model,  and predicting annotation masks.  </p> <p>Info</p> <p>In deep learning, the output of an Encoder model is called embeddings or features.</p> <p>You can bring up the Feature Extractor widget from the napari Plugins menu:  </p> <p></p>"},{"location":"feature_extractor/#widget-tools-description","title":"Widget Tools Description","text":"<ol> <li>Image Layer: To select your current image stack.</li> <li>Encoder Model: Sets which model you want to use for feature extraction.     The FF plugins, by default, comes with <code>MobileSAM</code>, <code>SAM (huge)</code>, <code>\u03bcSAM_LM (base)</code>, <code>\u03bcSAM_EM_Organelles (base)</code>, <code>DINOv2</code>, <code>SAM2 (large)</code>, and <code>SAM2 (base)</code> models. It is also possible to introduce a new model by adding a new model adapter class.</li> <li>No Patching: If checked, it means no patching will happen during the feature extraction process. Otherwise, the image will be divided into patches before passing them through the encoder model. By using SAM2 encoders, you might check this box to save some computation power.  </li> <li>Features Storage File: Where you want to save the features as an <code>HDF5</code> file.</li> <li>Extract Features button: Will run the feature extraction process.</li> <li>Stop button: To stop the extraction process!</li> </ol>"},{"location":"feature_extractor/#model-selection","title":"Model Selection","text":"<p>Our experiments tell us usually the <code>SAM2 (large)</code> model works the best. However, for less complicated images, using <code>MobileSAM</code> or <code>DINOv2</code> might also result in a good segmentation as they are lighter and faster.  </p> <p>Note</p> <p>When you use a model for the first time, the model's weight will be downloaded from their repository.  So, you might hit a little delay at the first usage of model.  </p> <p>Once you have your image features extracted, you can use the Segmentation widget to generate your image masks.</p>"},{"location":"howto/","title":"How to use the plugin","text":"<p>Microscopic images usually come with a large stack: many high-resolution slices!</p> <p>There are two ways to utilize this plugin over a large stack:</p> <ul> <li>One Model for All: Training an RF model on a small sub-stack, then predicting over the entire stack.</li> <li>Divide And Conquer: Dividing the large stack into several sub-stacks, then train an RF model for each.</li> </ul>"},{"location":"howto/#one-model-for-all","title":"One Model For All","text":"<p>As for the first step, we recommend making a small sub-stack to train a Random Forest (RF) model using our plugin. This sub-stack can have about 20 slices selected across the whole stack (not just the beginning or last few slices). This way, when you extract and save the sub-stack's features, the storage file won't occupy too much space on the hard drive.  </p> <p>Tip</p> <p>If the image resolution is high, it's better to down-scale images into a resolution of below 1200 pixels for the largest dimension.</p> <p>After the training, you can save the RF model, and later apply it on the entire stack.  </p>"},{"location":"howto/#divide-and-conquer","title":"Divide And Conquer","text":"<p>Extracted features saved as an\u00a0<code>HDF5</code>\u00a0file can take up a huge space on the disk. In this method, to prevent disk space overflow, you can divide your large stack into several sub-stacks. Then use the plugin for each, separately.  Although, you can try one trained model over another sub-stack, Random Forest model can not be fine-tuned. By using this method, you can achieve better annotations at the expense of spending more time on training several models.</p>"},{"location":"install/","title":"Installation","text":"<p>It is highly recomended to create and use a new environment to install the plugin and its dependencies. You can use mamba or conda to manage you environments but it's not necessary and you can use python <code>venv</code> as well. </p>"},{"location":"install/#setup","title":"Setup","text":"<p>We provided <code>install.sh</code> for Linux &amp; Mac OS users, and <code>install.bat</code> for Windows users. First you need to clone the repo: <pre><code>git clone https://github.com/juglab/featureforest\ncd ./featureforest\n</code></pre> Now run the installation script: <pre><code># Linux or Mac OS\nsh ./install.sh\n</code></pre> <pre><code># Windows\n./install.bat\n</code></pre></p>"},{"location":"install/#step-by-step","title":"Step by Step","text":"<ol> <li> <p>Create a new environment: <pre><code>conda create -n featureforest -y python=3.10\n</code></pre></p> </li> <li> <p>Activate the environment: <pre><code>conda activate featureforest\n</code></pre></p> </li> <li> <p>Install <code>torch</code> and <code>torchvision</code>: You can follow the instruction from here. But we can also use <code>light-the-torch</code> package: <pre><code>pip install light-the-torch\nltt install 'torch&gt;=2.5.1' 'torchvision&gt;=0.20.1'\n</code></pre> This will install the appropriate PyTorch binaries without user intervention by automatically identifying compatible CUDA versions from the local setup.  </p> </li> <li> <p>Installing all other dependencies: <pre><code>pip install -r ./requirements.txt\n</code></pre> This will install all dependencies including <code>napari</code>, <code>segment-anything</code> and <code>sam-2</code>.  </p> </li> <li> <p>Finally, install the plugin: <pre><code>pip install git+https://github.com/juglab/featureforest.git\n</code></pre></p> </li> </ol>"},{"location":"install/#requirements","title":"Requirements","text":"<ul> <li><code>python&gt;=3.10</code></li> <li><code>numpy&lt;2.2</code></li> <li><code>pytorch&gt;=2.5.1</code></li> <li><code>torchvision&gt;=0.20.1</code></li> <li><code>timm</code></li> <li><code>segment-anything</code></li> <li><code>sam-2</code></li> <li><code>opencv-python</code></li> <li><code>scikit-learn</code></li> <li><code>scikit-image</code></li> <li><code>scipy</code></li> <li><code>matplotlib</code></li> <li><code>pyqt</code></li> <li><code>magicgui</code></li> <li><code>qtpy</code></li> <li><code>napari</code></li> <li><code>h5py</code></li> <li><code>pynrrd</code></li> <li><code>pooch</code></li> </ul> <p>There is also a pypi package available that you can install FF using <code>pip</code>: <pre><code>pip install featureforest\n</code></pre></p> <p>Note</p> <p>Before install <code>featureforest</code> using <code>pip</code> you need to install <code>segment-anything</code> and <code>sam-2</code> manually.</p> <p>For detailed napari installation see here.  </p>"},{"location":"model_adapter/","title":"Adapting New Models","text":"<p>Feature Forest (FF) uses vision foundation models for the feature extraction.  <code>MobileSAM</code>, <code>SAM (huge)</code>, <code>\u03bcSAM_LM (base)</code>, <code>\u03bcSAM_EM_Organelles (base)</code>, <code>DINOv2</code>, <code>SAM2 (large)</code>, and <code>SAM2 (base)</code> models are already included into the plugin.</p> <p>Tip</p> <p>Based on our experiences, we found <code>SAM2</code> works better, especially for complex images.  If you have less complex stack to segment, you might get a decent result using <code>DINOv2</code> or even <code>MobileSAM</code> while they are lighter models.</p>"},{"location":"model_adapter/#adapting-a-new-model","title":"Adapting a new Model","text":"<p>New and powerful deep Learning models appear quite often, and we always want to try them over our data and tasks.  In FF, we provided an <code>BaseModelAdapter</code> class to adapt any model of interest. For adding a new model, you need to subclass the <code>BaseModelAdapter</code>, and set a few attributes and implement a few methods especially the <code>get_features_patches</code> method which does the feature extraction. The input for the model will be provided as image patches, but this will be handled by the plugin, so you can think of it as a batch of images. You also need to take care of input/output transformation by setting the right sizes for the <code>self.input_transforms</code> to make the right size input for the model, and the <code>self.embedding_transform</code> for resizing the model output back to the patch size. You can check the code for the <code>SAM2</code> model adapter here.  </p>"},{"location":"run_pipeline/","title":"Run Pipeline","text":"<p>After training your RF model in napari, you can use <code>run_pipeline.py</code> to run the pipeline on a new set of images without using napari gui and only by commandline. This allows you to run the whole pipeline on HPC or other servers as a batch job.</p>"},{"location":"run_pipeline/#usage","title":"Usage","text":"<pre><code>python run_pipeline.py -h\n</code></pre> <pre><code>FeatureForest run-pipeline script\n\noptions:\n  -h, --help            show this help message and exit\n  --data DATA           Path to the input image\n  --outdir OUTDIR       Path to the output directory\n  --rf_model RF_MODEL   Path to the trained RF model\n  --feat_model {SAM2_Large,SAM2_Base,\u03bcSAM_LM,\u03bcSAM_EM_Organelles,Cellpose_cyto3,MobileSAM,SAM,DinoV2}\n                        Name of the model for feature extraction\n  --no_patching         If true, no patching will be used during feature extraction\n  --smoothing_iterations SMOOTHING_ITERATIONS\n                        Post-processing smoothing iterations; default=25\n  --area_threshold AREA_THRESHOLD\n                        Post-processing area threshold to remove small regions; default=50\n  --post_sam            to use SAM2 for generating final masks\n  --only_extract        to only extract features to zarr file without running prediction pipeline\n</code></pre> <p>For example if you just want to extract features from a stack, using SAM2 Large model with no patching: <pre><code>python run_pipeline.py \\\n--data /path/to/input.tif \\\n--outdir /path/to/output/ \\\n--feat_model SAM2_Large \\\n--no_patching \\\n--only_extract\n</code></pre></p> <p>Another example, to run the whole pipeline (feature extraction, prediction, post-processing) on a stack using a trained RF model: <pre><code>python run_pipeline.py \\\n--data /path/to/input.tif \\\n--outdir /path/to/output/ \\\n--rf_model /path/to/trained/rf_model.bin \\\n--feat_model SAM2_Large\n--area_threshold 10 \\\n--post_sam\n</code></pre></p>"},{"location":"segmentation/","title":"Segmentation Widget","text":"<p>Hurray! Now you have your features extracted and ready for the main action! \ud83d\ude0a The Segmentation widget is a long widget with several panels, but don't worry we'll go through all of them, from top to bottom!  </p>"},{"location":"segmentation/#inputs-and-labels-statistics","title":"Inputs and Labels' statistics","text":""},{"location":"segmentation/#inputs","title":"Inputs","text":"<ol> <li>Input Layer: To set which napari layer is your input image layer</li> <li>Feature Storage: Select your previously extracted features <code>HDF5</code> file here. Note: You need to select the storage file for the selected input image, obviously!</li> <li>Ground Truth Layer: To select your Labels layer</li> <li>Add Layer button: To add a new GT layer to napari layers</li> </ol> <p>Note</p> <p>For annotations, users are expected to label pixels in the <code>Labels</code> layer and provide annotations as scribbles for multiple classes. This can be done by selecting the <code>paint brush</code> tool and choosing a specific <code>label</code> id for each class. An example screenshot has been provided to show how to provide annotations for, eg. the background class and foreground (nuclei) class.</p> <p></p>"},{"location":"segmentation/#labeling-statistics","title":"Labeling Statistics","text":"<ol> <li>Analyze button: To get info about number of classes and labels you've added so far.</li> </ol> <p>Note</p> <ul> <li>You can have as many Labels layer as you want. But only the selected one will be used for training the RF model.  </li> <li>You can also drag &amp; drop your previously saved labels into the napari and select that layer.</li> </ul>"},{"location":"segmentation/#training-the-rf-model","title":"Training the RF Model","text":"Train Model (Random Forest) <ol> <li>Number of Trees: To set number of trees (estimators) in the forest</li> <li>Max depth: The maximum depth of a tree</li> <li>Train button: To extract the training data and train the RF model</li> <li>Load Model button: Using this, you can load a previously trained and saved model.</li> <li>Save Model button: To save the current RF model</li> </ol> <p>Tip</p> <ul> <li>Setting a high value for the <code>Max depth</code> would overfit your RF model over the training data. So, it won't perform well on test images. But if you're doing the segmentation over the entire stack (or a single image), you may try higher values.</li> </ul>"},{"location":"segmentation/#prediction","title":"Prediction","text":""},{"location":"segmentation/#segmentation-layer","title":"Segmentation Layer","text":"<ol> <li>New Layer: If checked, the segmentation result will show up on a new layer in napari</li> <li>Layer Dropdown: You can select which layer should be used as the layer for the segmentation result</li> <li>Add/Replace Segmentation option: Based on your choice, this will add new segmentation to the previous result, or completely replace the result (Default).</li> </ol>"},{"location":"segmentation/#buttons","title":"Buttons","text":"<ol> <li>Predict Slice button: To generate the segmentation mask for the current slice</li> <li>Predict Whole Stack button: to start the prediction process for the whole loaded stack</li> <li>Stop button: Just for your safety!\ud83d\ude09 this will stop the prediction process over the whole stack.</li> </ol>"},{"location":"segmentation/#post-processing","title":"Post-processing","text":"<ol> <li>Smoothing Iterations: Sets how many times the min-curvature smoothing algorithm should be run.</li> <li>Area Threshold: You can set a threshold to remove very small regions with area below this threshold. It can be an absolute pixel based area, or a relative percentage value (100% will be the largest region in the mask).</li> <li>Use SAM Predictor: To use SAM as the mask predictor. By checking this option, a bounding box around each RF generated mask will be feeded as a prompt to SAM, and SAM will predict the final mask.</li> <li>Use SAM Auto-Segmentation: By choosing this option, first, SAM auto-segmentation process predicts all possible mask components in the image, then those components with high IOU over the RF predicted components will be selected as the final mask.<ul> <li>IOU Matching Threshold: The threshold to match SAM auto-segmentation masks with the RF predicted ones. SAM generated components with IOU below this threshold will be ignored.</li> </ul> </li> <li>Apply to Slice button: This will run the post-processing and apply it to the mask.</li> <li>Apply to Stack button: To apply the post-processing to the whole loaded stack, please push this button!</li> </ol> <p>Info</p> <p>RF prediction is pixel based, so the mask will have holes, fuzzy edges and it's not smooth. To make the predicted mask smooth, we are using the min-curvature smoothing algorhitm which is a edge-preserving iterative algorithm. It means, this algorithm will not change the overall shape of the mask components while smoothing them, and you have the power to run it enough times to get your desired results.</p> <p>Tip</p> <p>Using SAM for the post-processing usually generate a decent mask. If your image has small number of almost well-separated objects, then we recommend using the SAM Auto-Segmentation option. Otherwise, use the SAM Predictor. It will be faster as well.</p>"},{"location":"segmentation/#export","title":"Export","text":"<p> With this panel, you can export the predicted mask into provided formats.  </p> <ol> <li>Export Format: To select the export file format. Currently TIFF, NRRD, NUMPY are supported.</li> <li>Export with Post-processing: If checked you'll export the post-processed mask.</li> <li>Export button: Open up a save dialog to get where to save the file.</li> </ol>"},{"location":"segmentation/#run-prediction-pipeline","title":"Run Prediction Pipeline","text":"<p> When you are happy with the trained RF model, you can use this panel to run the prediction over the whole large stack that you may have.  </p> <ol> <li>Select your stack: Set your large stack here.</li> <li>Result Directory: To se where to save the prediction results</li> <li>Run Prediction button: To run the process and go for a drink!</li> <li>Stop button: In case of any regrets or forgetfulness, use this!</li> </ol> <p>Info</p> <p>Results will be saved image-by-image as TIFF files. Also, all the set post-processing will be applied and saved in separate folders.</p>"},{"location":"tutorial/","title":"Labeling Tutorial","text":"<p>Here, we provide a simple tutorial on how to use FeatureForest (FF). We assume you have already extracted and saved your train image/stack features using the Feature Extractor widget.</p>"},{"location":"tutorial/#lets-begin","title":"Let's Begin!","text":"<p>Load your image and open the Segmentation widget. The first step is to choose your image features HDF5 file. Click on the \"Select...\" button and select your HDF5 file.</p> <p>Note</p> <p>If you loaded more than one image/stack, you need to select your target image from the \"Input Layer\" dropdown.</p> <p></p> <p>The next step is to add a label layer. Use the \"Add Layer\" button. You can have more than one label layer. However, only the selected one is the effective label layer for training the RF model. Now, it's time to add some scribble labels. Select the label layer and then use the brush tool from the layer's top toolbar. Always use the first class \"1\" for labeling your background (areas you don't want to create a mask for). You can change the class and add labels for the next class, which are objects of interest (areas that you want to create masks for) by increasing the label number. If you make a mistake, you can remove the labels by selecting the eraser button and removing the wrong labels.</p> <p>Note</p> <ul> <li>Always make sure you select the label layer before using the brush tool.</li> <li>Class 1 must always be assigned to the background. We use this constraint to make sure the final mask won't include the background and to make it transparent on the napari viewer.</li> </ul> <p>After adding some labels, use the \"Train RF Model\" button to train the RF model. You can set the number of trees and the tree's max depth. But be careful not to set a high value for the max depth parameter. It will result in an over-fitted RF model, and it wouldn't perform well over images not presented in your training stack. When the training is done, use the \"Predict Slice\" button to make your first segmentation mask. We know the first one is usually not very good \ud83d\ude41, but don't get disappointed! Good results will come with consistency, so do some more iterations \ud83d\ude0a.</p> <p></p> <p>Tip</p> <p>Add some labels for background and other classes near the boundaries of targeted objects. This way you provide the most distinctive information for the model.</p> <p>Don't add too many labels!</p> <p>Training a model is an iterative process, so add as few labels as possible at each iteration. Then, train the RF model, make a prediction, and repeat the process by adding a bit more labels over the areas where the model made mistakes. For labeling, you can use any slices in the training stack. Therefore, it's better to make predictions over the whole stack and add more labels on the slices in which the model performs poorly.</p> <p>Tip</p> <p>The default brush size is set to one. We recommended using very thin labels to avoid annotating unnecessary pixels. Neighboring pixels usually have similar features, which won't provide distinctive information for the training RF model.</p>"},{"location":"tutorial/#what-about-the-post-processing","title":"What about the post-processing?","text":"<p>Post-processing is kind of our winning card! Since we are using a RF model for pixel classification, the produced masks usually have holes and fuzzy edges, and post-processing is there to fix that!</p>"},{"location":"tutorial/#simple-post-processing","title":"Simple post-processing:","text":"<p>The postprocessing will use a simple morphology and mean-curvature smoothing to fill holes and smooth the edges. You can set the number of smoothing iterations. The next step is to eliminate small noisy areas that should not be included in the mask (usually from the background class). You can set the area threshold for that, and all small regions with an area below the threshold will be excluded from the mask.</p>"},{"location":"tutorial/#post-processing-using-sam2","title":"Post-processing using SAM2","text":"<p>If you select the \"Use SAM Predictor\" option, bounding boxes around the regions of the RF-predicted mask will be used as prompts to SAM2. Then you will get a nice smooth mask as if you provided those prompts to SAM2. Smart, don't you agree?! \ud83d\ude01 The \"Use SAM Auto-Segmentation\" option will use SAM2 auto-segmentation prediction and then select regions that intersect with the RF mask above the set threshold. We recommend using this option only if you have not too many objects in the image. Otherwise, it will be slow and take some time to make the final mask.</p> <p>Info</p> <ul> <li> <p>Use the \"Export\" button to save the created mask. This way the labels will be 0 for the background and onwards for other classes. If you save the mask using the napari file menu, labels will be started from 1 and on...</p> </li> <li> <p>Every time you make a prediction, the result will appear on a new layer unless you uncheck the \"New Layer\" checkbox.</p> </li> </ul> <p>Thanks for using FF, and have a happy experience! \ud83d\ude4c</p>"}]}